{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "Ry_4vW0w_BN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A2hvJmoE4d6R"
      },
      "outputs": [],
      "source": [
        "# Pytorch Dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# RF from sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network with 1 hidden layer"
      ],
      "metadata": {
        "id": "RDdKkNqd_G8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple MLP (Multi-Layer Perceptron)\n",
        "class ScalingFactorPredictor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)  # First layer\n",
        "    self.relu = nn.ReLU()                          # Activation function (can try different ones)\n",
        "    self.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.fc1(x)\n",
        "      out = self.relu(out)\n",
        "      out = self.fc2(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "nTx46js75SV2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepping the Data"
      ],
      "metadata": {
        "id": "3dlRuZwNZtxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalingFactorDataset(Dataset):\n",
        "\n",
        "  def __init__(self, csvfile):\n",
        "\n",
        "    self.data = pd.read_csv(csvfile)\n",
        "\n",
        "    x = self.data[\"p\"].values\n",
        "    y = self.data[\"mean best k\"].values\n",
        "\n",
        "    self.x = torch.tensor(x, dtype=torch.float32)\n",
        "    self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx].unsqueeze(0), self.y[idx].unsqueeze(0)"
      ],
      "metadata": {
        "id": "tqEDJj-lZtVd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "Xj2ih6DT_M07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest regression model\n",
        "rf_reg = RandomForestRegressor(n_estimators=100)"
      ],
      "metadata": {
        "id": "YYYpcC_B-Ss3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Neural Network"
      ],
      "metadata": {
        "id": "ilV_ZQR7_AbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, epochs, criterion, optimizer, device):\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for x_batch, y_batch in dataloader:\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "      pred = model(x_batch)\n",
        "      loss = criterion(pred, y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x_batch, y_batch in dataloader:\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      pred = model(x_batch)\n",
        "      loss = criterion(pred, y_batch)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "  avg_loss = test_loss / len(dataloader)\n",
        "  print(f\"Test Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "R6r7YnByMznD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1     # Input features (probability p)\n",
        "hidden_size = 4    # Number of neurons in the hidden layer\n",
        "output_size = 1    # Output features (scaling factor)\n",
        "\n",
        "model = ScalingFactorPredictor(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # GD Optimizer\n",
        "\n",
        "# create dataset and randomly split into training, validation, and test datasets\n",
        "dataset = ScalingFactorDataset(\"best_k_data.csv\")\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size  # Ensures full coverage\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# initialize dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False)"
      ],
      "metadata": {
        "id": "yieXKXLQ-V3t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "train_model(model, train_loader, epochs=100, criterion=criterion, optimizer=optimizer, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHj5D-MCU0xu",
        "outputId": "8f2f671a-59a5-49b8-acda-ac297595fbde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.0479\n",
            "Epoch 2/100, Loss: 0.0453\n",
            "Epoch 3/100, Loss: 0.0430\n",
            "Epoch 4/100, Loss: 0.0408\n",
            "Epoch 5/100, Loss: 0.0388\n",
            "Epoch 6/100, Loss: 0.0370\n",
            "Epoch 7/100, Loss: 0.0353\n",
            "Epoch 8/100, Loss: 0.0339\n",
            "Epoch 9/100, Loss: 0.0323\n",
            "Epoch 10/100, Loss: 0.0312\n",
            "Epoch 11/100, Loss: 0.0300\n",
            "Epoch 12/100, Loss: 0.0290\n",
            "Epoch 13/100, Loss: 0.0280\n",
            "Epoch 14/100, Loss: 0.0272\n",
            "Epoch 15/100, Loss: 0.0264\n",
            "Epoch 16/100, Loss: 0.0258\n",
            "Epoch 17/100, Loss: 0.0252\n",
            "Epoch 18/100, Loss: 0.0246\n",
            "Epoch 19/100, Loss: 0.0241\n",
            "Epoch 20/100, Loss: 0.0237\n",
            "Epoch 21/100, Loss: 0.0233\n",
            "Epoch 22/100, Loss: 0.0230\n",
            "Epoch 23/100, Loss: 0.0226\n",
            "Epoch 24/100, Loss: 0.0224\n",
            "Epoch 25/100, Loss: 0.0221\n",
            "Epoch 26/100, Loss: 0.0219\n",
            "Epoch 27/100, Loss: 0.0217\n",
            "Epoch 28/100, Loss: 0.0215\n",
            "Epoch 29/100, Loss: 0.0214\n",
            "Epoch 30/100, Loss: 0.0213\n",
            "Epoch 31/100, Loss: 0.0211\n",
            "Epoch 32/100, Loss: 0.0210\n",
            "Epoch 33/100, Loss: 0.0209\n",
            "Epoch 34/100, Loss: 0.0209\n",
            "Epoch 35/100, Loss: 0.0208\n",
            "Epoch 36/100, Loss: 0.0207\n",
            "Epoch 37/100, Loss: 0.0206\n",
            "Epoch 38/100, Loss: 0.0206\n",
            "Epoch 39/100, Loss: 0.0205\n",
            "Epoch 40/100, Loss: 0.0205\n",
            "Epoch 41/100, Loss: 0.0204\n",
            "Epoch 42/100, Loss: 0.0204\n",
            "Epoch 43/100, Loss: 0.0203\n",
            "Epoch 44/100, Loss: 0.0203\n",
            "Epoch 45/100, Loss: 0.0203\n",
            "Epoch 46/100, Loss: 0.0202\n",
            "Epoch 47/100, Loss: 0.0202\n",
            "Epoch 48/100, Loss: 0.0202\n",
            "Epoch 49/100, Loss: 0.0201\n",
            "Epoch 50/100, Loss: 0.0201\n",
            "Epoch 51/100, Loss: 0.0201\n",
            "Epoch 52/100, Loss: 0.0200\n",
            "Epoch 53/100, Loss: 0.0200\n",
            "Epoch 54/100, Loss: 0.0200\n",
            "Epoch 55/100, Loss: 0.0200\n",
            "Epoch 56/100, Loss: 0.0199\n",
            "Epoch 57/100, Loss: 0.0199\n",
            "Epoch 58/100, Loss: 0.0199\n",
            "Epoch 59/100, Loss: 0.0198\n",
            "Epoch 60/100, Loss: 0.0198\n",
            "Epoch 61/100, Loss: 0.0198\n",
            "Epoch 62/100, Loss: 0.0198\n",
            "Epoch 63/100, Loss: 0.0197\n",
            "Epoch 64/100, Loss: 0.0197\n",
            "Epoch 65/100, Loss: 0.0197\n",
            "Epoch 66/100, Loss: 0.0197\n",
            "Epoch 67/100, Loss: 0.0196\n",
            "Epoch 68/100, Loss: 0.0196\n",
            "Epoch 69/100, Loss: 0.0196\n",
            "Epoch 70/100, Loss: 0.0196\n",
            "Epoch 71/100, Loss: 0.0195\n",
            "Epoch 72/100, Loss: 0.0195\n",
            "Epoch 73/100, Loss: 0.0195\n",
            "Epoch 74/100, Loss: 0.0195\n",
            "Epoch 75/100, Loss: 0.0194\n",
            "Epoch 76/100, Loss: 0.0194\n",
            "Epoch 77/100, Loss: 0.0194\n",
            "Epoch 78/100, Loss: 0.0194\n",
            "Epoch 79/100, Loss: 0.0194\n",
            "Epoch 80/100, Loss: 0.0193\n",
            "Epoch 81/100, Loss: 0.0193\n",
            "Epoch 82/100, Loss: 0.0193\n",
            "Epoch 83/100, Loss: 0.0192\n",
            "Epoch 84/100, Loss: 0.0192\n",
            "Epoch 85/100, Loss: 0.0192\n",
            "Epoch 86/100, Loss: 0.0192\n",
            "Epoch 87/100, Loss: 0.0192\n",
            "Epoch 88/100, Loss: 0.0191\n",
            "Epoch 89/100, Loss: 0.0191\n",
            "Epoch 90/100, Loss: 0.0191\n",
            "Epoch 91/100, Loss: 0.0191\n",
            "Epoch 92/100, Loss: 0.0190\n",
            "Epoch 93/100, Loss: 0.0190\n",
            "Epoch 94/100, Loss: 0.0190\n",
            "Epoch 95/100, Loss: 0.0190\n",
            "Epoch 96/100, Loss: 0.0190\n",
            "Epoch 97/100, Loss: 0.0189\n",
            "Epoch 98/100, Loss: 0.0189\n",
            "Epoch 99/100, Loss: 0.0189\n",
            "Epoch 100/100, Loss: 0.0189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "evaluate_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGOQ8maViPN",
        "outputId": "17dc2a6e-7dfd-40ed-ee2e-d929b273fa56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Random Forest:"
      ],
      "metadata": {
        "id": "W5HWMnIA_9Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(<x_train>, <y_train>)"
      ],
      "metadata": {
        "id": "EAF2P75kAAeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}